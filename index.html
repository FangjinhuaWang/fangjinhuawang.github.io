<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Fangjinhua Wang</title>
  
  <meta name="author" content="Fangjinhua Wang">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
	<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Fangjinhua Wang (ÁéãÊñπÈî¶Âçé)</name>
              </p>
              <p>I am a final-year Ph.D. student in Computer Science at <a href="https://ethz.ch/en.html">ETH Zurich</a>, supervised by Prof. <a href="https://cvg.ethz.ch/team/Prof-Dr-Marc-Pollefeys">Marc Pollefeys</a>. 
              
                Previously, I obtained a Master‚Äôs degree in Robotics at ETH zurich (with distinction) and a Bachelor's degree in Mechatronics Engineering at Zhejiang University (with distinction). I am the recipient of ETH Medal (ETH Zurich, 2021) and Chu Kochen Award (highest honor at Zhejiang University, 2017). 
              </p>
              <p>

              </p>
              <p>
                <font color="red"><strong>
                  I am on the job market. 
                </strong></font>
              </p>
              <p style="text-align:center">
                <a href="mailto:fangjinhua.wang@inf.ethz.ch">Email</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=ysTmrEsAAAAJ">Google Scholar</a> &nbsp/&nbsp
                <a href="https://github.com/FangjinhuaWang">Github</a> &nbsp/&nbsp
                <a href="https://www.linkedin.com/in/fangjinhua-wang-4ba2aa150/">Linkedin</a> &nbsp/&nbsp
                <a href="https://twitter.com/FangjinhuaWang">Twitter</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/Fangjinhua_Wang_photo.png"><img style="width:80%;max-width:80%" alt="profile photo" src="images/Fangjinhua_Wang_photo.png" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research Overview</heading>
              <p>
                My research is motivated by the goal of enabling embodied AI agents to perceive, understand and interact with the physical world as humans. During my PhD, I have been working in the intersection of 3D computer vision and deep learning, including perception of spatial positioning information (visual localization), geometry (3D reconstruction) and appearance (neural rendering). 
              </p>
              <p>
                On the one hand, I focus on efficient algorithms that are computation-friendly for AR/VR/MR/robotics in general settings. On the other hand, I am interested in accurate representation for challenging settings, including large-scale scenes, non-Lambertian surfaces, dynamics and sparse inputs. 
              </p>
              <p>
                Currently, I am working on understanding and interaction with the physical world, such as open vocabulary scene understanding. 
              </p>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Publications</heading>
              <p>
                * denotes equal contribution. &#10013 denotes corresponding author.
              </p>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
				  
          <tr onmouseout="openfungraph_stop()" onmouseover="openfungraph_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='openfungraph_image'>
                  <img src='images/openfungraph_after.png' width="180"></div>
                <img src='images/openfungraph_before.png' width="180">
              </div>
              <script type="text/javascript">
                function openfungraph_start() {
                  document.getElementById('openfungraph_image').style.opacity = "1";
                }

                function openfungraph_stop() {
                  document.getElementById('openfungraph_image').style.opacity = "0";
                }
                openfungraph_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="">
                <papertitle>Open-Vocabulary Functional 3D Scene Graphs for Real-World Indoor Spaces</papertitle>
              </a>
              <br>
              Chenyangguang Zhang,
              Alexandros Delitzas,
              <strong>Fangjinhua Wang</strong>,
              Ruida Zhang,
              Xiangyang Ji,
              Marc Pollefeys,
               Francis Engelmann
              <br>
              <em>CVPR</em>, 2025
              <br>
              <a href="">Paper</a>
              <p></p>
            </td>
          </tr>   

          <tr onmouseout="rscore_stop()" onmouseover="rscore_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='rscore_image'>
                  <img src='images/rscore_after.png' width="180"></div>
                <img src='images/rscore_before.png' width="180">
              </div>
              <script type="text/javascript">
                function rscore_start() {
                  document.getElementById('rscore_image').style.opacity = "1";
                }

                function rscore_stop() {
                  document.getElementById('rscore_image').style.opacity = "0";
                }
                rscore_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/2501.01421">
                <papertitle>R-SCoRe: Revisiting Scene Coordinate Regression for Robust Large-Scale Visual Localization</papertitle>
              </a>
              <br>
              Xudong Jiang,
              <strong>Fangjinhua Wang&#10013</strong>,
              Silvano Galliani, 
              Christoph Vogel, 
              Marc Pollefeys
              <br>
              <em>CVPR</em>, 2025
              <br>
              <a href="https://arxiv.org/pdf/2501.01421">Paper</a> /
              <a href="https://github.com/cvg/scrstudio">Code</a>
              <p></p>
            </td>
          </tr>   

          <tr onmouseout="depthsplat_stop()" onmouseover="depthsplat_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='depthsplat_image'>
                  <video width="120%" muted="" autoplay="" loop="">
                    <source src="images/depthsplat.mp4" type="video/mp4">
                  </video>
                  </div>
                <img src='images/depthsplat.jpg' width="120%">
              </div>
              <script type="text/javascript">
                function depthsplat_start() {
                  document.getElementById('depthsplat_image').style.opacity = "1";
                }

                function depthsplat_stop() {
                  document.getElementById('depthsplat_image').style.opacity = "0";
                }
                depthsplat_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2410.13862">
                <papertitle>DepthSplat: Connecting Gaussian Splatting and Depth</papertitle>
              </a>
              <br>
              Haofei Xu,
              Songyou Peng,
              <strong>Fangjinhua Wang</strong>,
              Hermann Blum,
              Daniel Barath,
              Andreas Geiger,
              Marc Pollefeys
              <br>
              <em>CVPR</em>, 2025
              <br>
              <a href="https://arxiv.org/abs/2410.13862">Paper</a> /
              <a href="https://haofeixu.github.io/depthsplat/">Project Page</a> /
              <a href="https://github.com/cvg/depthsplat">Code</a>
              <p></p>
            </td>
          </tr>

          <tr onmouseout="mvs_survey_stop()" onmouseover="mvs_survey_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='mvs_survey_image'>
                  <img src='images/mvs_survey_after.png' width="180"></div>
                <img src='images/mvs_survey_before.png' width="180">
              </div>
              <script type="text/javascript">
                function mvs_survey_start() {
                  document.getElementById('mvs_survey_image').style.opacity = "1";
                }

                function mvs_survey_stop() {
                  document.getElementById('mvs_survey_image').style.opacity = "0";
                }
                mvs_survey_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/2408.15235">
                <papertitle>Learning-based Multi-View Stereo: A Survey</papertitle>
              </a>
              <br>
              <strong>Fangjinhua Wang*</strong>,
              Qingtian Zhu*, 
              Di Chang*, 
              Quankai Gao, 
              Junlin Han, 
              Tong Zhang, 
              Richard Hartley, 
              Marc Pollefeys&#10013
              <br>
              <em>In arXiv. Under review</em>
              <br>
              <a href="https://arxiv.org/pdf/2408.15235">Paper</a>
              <p></p>
            </td>
          </tr>   

          <tr onmouseout="unisdf_stop()" onmouseover="unisdf_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='unisdf_image'>
                  <img src='images/unisdf_after.png' width="180"></div>
                <img src='images/unisdf_before.png' width="180">
              </div>
              <script type="text/javascript">
                function unisdf_start() {
                  document.getElementById('unisdf_image').style.opacity = "1";
                }

                function unisdf_stop() {
                  document.getElementById('unisdf_image').style.opacity = "0";
                }
                unisdf_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://fangjinhuawang.github.io/UniSDF/">
                <papertitle>UniSDF: Unifying Neural Representations for High-Fidelity 3D Reconstruction of Complex Scenes with Reflections</papertitle>
              </a>
              <br>
              <strong>Fangjinhua Wang</strong>,
              Marie-Julie Rakotosaona,
              Michael Niemeyer,
              Richard Szeliski,
              Marc Pollefeys,
              Federico Tombari
              <br>
              <em>NeurIPS</em>, 2024
              <br>
              <a href="https://arxiv.org/abs/2312.13285">Paper</a> /
              <a href="https://fangjinhuawang.github.io/UniSDF/">Project Page</a> /
              <a href="">Code (coming soon)</a>
              <p></p>
            </td>
          </tr>   

          <tr onmouseout="hsr_stop()" onmouseover="hsr_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='hsr_image'>
                  <img src='images/hsr_after.png' width="180"></div>
                <img src='images/hsr_before.png' width="180">
              </div>
              <script type="text/javascript">
                function hsr_start() {
                  document.getElementById('hsr_image').style.opacity = "1";
                }

                function hsr_stop() {
                  document.getElementById('hsr_image').style.opacity = "0";
                }
                hsr_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://lxxue.github.io/human-scene-recon/">
                <papertitle>HSR: Holistic 3D Human-Scene Reconstruction from Monocular Videos</papertitle>
              </a>
              <br>
              Lixin Xue, 
              Chen Guo, 
              Chengwei Zheng,
              <strong>Fangjinhua Wang</strong>,
              Tianjian Jiang,
              Hsuan-I Ho, 
              Manuel Kaufmann, 
              Jie Song, 
              Otmar Hilliges
              <br>
              <em>ECCV</em>, 2024
              <br>
              <a href="https://files.ait.ethz.ch/projects/hsr/hsr_paper_main.pdf">Paper</a> /
              <a href="https://lxxue.github.io/human-scene-recon/">Project Page</a> /
              <a href="https://lxxue.github.io/human-scene-recon/">Code</a>
              <p></p>
            </td>
          </tr>   

          <tr onmouseout="glace_stop()" onmouseover="glace_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='glace_image'>
                  <img src='images/glace_after.png' width="180"></div>
                <img src='images/glace_before.png' width="160">
              </div>
              <script type="text/javascript">
                function glace_start() {
                  document.getElementById('glace_image').style.opacity = "1";
                }

                function glace_stop() {
                  document.getElementById('glace_image').style.opacity = "0";
                }
                glace_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://xjiangan.github.io/glace">
                <papertitle>GLACE: Global Local Accelerated Coordinate Encoding</papertitle>
              </a>
              <br>
              <strong>Fangjinhua Wang*</strong>,
              Xudong Jiang*,
              Silvano Galliani, 
              Christoph Vogel, 
              Marc Pollefeys
              <br>
              <em>CVPR</em>, 2024
              <br>
              <a href="https://arxiv.org/abs/2406.04340">Paper</a> /
              <a href="https://xjiangan.github.io/glace">Project Page</a> /
              <a href="https://github.com/cvg/glace">Code</a>
              <p></p>
            </td>
          </tr>   

		      <tr onmouseout="volrecon_stop()" onmouseover="volrecon_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='volrecon_image'>
                  <img src='images/volrecon_after.jpg' width="180"></div>
                <img src='images/volrecon_before.jpg' width="180">
              </div>
              <script type="text/javascript">
                function volrecon_start() {
                  document.getElementById('volrecon_image').style.opacity = "1";
                }

                function volrecon_stop() {
                  document.getElementById('volrecon_image').style.opacity = "0";
                }
                volrecon_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2212.08067">
                <papertitle>VolRecon: Volume Rendering of Signed Ray Distance Functions for Generalizable Multi-View Reconstruction</papertitle>
              </a>
              <br>
              Yufan Ren*,
              <strong>Fangjinhua Wang*</strong>,
              Tong Zhang,
              Marc Pollefeys,
              Sabine S√ºsstrunk
              <br>
              <em>CVPR</em>, 2023
              <br>
              <a href="https://arxiv.org/abs/2212.08067">Paper</a> /
              <a href="https://fangjinhuawang.github.io/VolRecon/">Project Page</a> /
              <a href="https://github.com/IVRL/VolRecon">Code</a>
              <p></p>
            </td>
          </tr>   

          <tr onmouseout="itermvs_stop()" onmouseover="itermvs_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='itermvs_image'>
                  <img src='images/itermvs_after.jpg' width="160"></div>
                <img src='images/itermvs_before.jpg' width="160">
              </div>
              <script type="text/javascript">
                function itermvs_start() {
                  document.getElementById('itermvs_image').style.opacity = "1";
                }

                function itermvs_stop() {
                  document.getElementById('itermvs_image').style.opacity = "0";
                }
                itermvs_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Wang_IterMVS_Iterative_Probability_Estimation_for_Efficient_Multi-View_Stereo_CVPR_2022_paper.pdf">
                <papertitle>IterMVS: Iterative Probability Estimation for Efficient Multi-View Stereo</papertitle>
              </a>
              <br>
              <strong>Fangjinhua Wang</strong>,
              Silvano Galliani, 
              Christoph Vogel, 
              Marc Pollefeys
              <br>
              <em>CVPR</em>, 2022
              <br>
              <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Wang_IterMVS_Iterative_Probability_Estimation_for_Efficient_Multi-View_Stereo_CVPR_2022_paper.pdf">Paper</a> /
              <a href="https://github.com/FangjinhuaWang/IterMVS">Code</a>
              <p></p>
            </td>
          </tr>		
          

          <tr onmouseout="patchmatchnet_stop()" onmouseover="patchmatchnet_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='patchmatchnet_image'>
                  <img src='images/patchmatchnet_after.jpg' width="180"></div>
                <img src='images/patchmatchnet_before.jpg' width="180">
              </div>
              <script type="text/javascript">
                function patchmatchnet_start() {
                  document.getElementById('patchmatchnet_image').style.opacity = "1";
                }

                function patchmatchnet_stop() {
                  document.getElementById('patchmatchnet_image').style.opacity = "0";
                }
                patchmatchnet_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Wang_PatchmatchNet_Learned_Multi-View_Patchmatch_Stereo_CVPR_2021_paper.pdf">
                <papertitle>PatchmatchNet: Learned Multi-View Patchmatch Stereo</papertitle>
              </a>
              <br>
              <strong>Fangjinhua Wang</strong>,
              Silvano Galliani, 
              Christoph Vogel, 
              Pablo Speciale, 
              Marc Pollefeys
              <br>
              <em>CVPR</em>, 2021 &nbsp <font color="red"><strong>(Oral Presentation)</strong></font>
              <br>
              <a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Wang_PatchmatchNet_Learned_Multi-View_Patchmatch_Stereo_CVPR_2021_paper.pdf">Paper</a> /
              <a href="https://github.com/FangjinhuaWang/PatchmatchNet">Code</a>
              <p></p>
            </td>
          </tr>   


          <tr onmouseout="multisensorwearable_stop()" onmouseover="multisensorwearable_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='multisensorwearable_image'>
                  <img src='images/multisensorwearable_after.jpg' width="180"></div>
                <img src='images/multisensorwearable_before.jpg' width="180">
              </div>
              <script type="text/javascript">
                function multisensorwearable_start() {
                  document.getElementById('multisensorwearable_image').style.opacity = "1";
                }

                function multisensorwearable_stop() {
                  document.getElementById('multisensorwearable_image').style.opacity = "0";
                }
                multisensorwearable_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://ieeexplore.ieee.org/abstract/document/9628050/">
                <papertitle>Self-Calibrated Multi-Sensor Wearable for Hand Tracking and Modeling</papertitle>
              </a>
              <br>
              Nikhil Bharadwaj Gosala*,
              <strong>Fangjinhua Wang*</strong>,
              Zhaopeng Cui, 
              Hanxue Liang, 
              Oliver Glauser, 
              Shihao Wu, 
              Olga Sorkine-Hornung
              <br>
              <em>IEEE Transactions on Visualization and Computer Graphics</em>, 2021
              <p></p>
            </td>
          </tr>   
         

        </tbody></table>

				<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Teaching experience</heading>
            </td>
          </tr>
        </tbody></table>
        <table width="100%" align="center" border="0" cellpadding="20"><tbody>
          
          <tr>
            <td width="75%" valign="center">
              <strong>Computer Vision</strong>, ETH Zurich, Autumn 2021, Autumn 2022, Autumn 2023, Autumn 2024
              <br>
              <strong>3D Vision</strong>, ETH Zurich, Spring 2023, Spring 2024
              <br>
              <strong>Introduction to Machine Learning</strong>, ETH Zurich, Spring 2021
            </td>
          </tr>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Work experience</heading>
            </td>
          </tr>
        </tbody></table>
        <table width="100%" align="center" border="0" cellpadding="20"><tbody>
					
          <tr>
            <td width="75%" valign="center">
              <strong>Google</strong> (Zurich, Switzerland), Student Researcher, 06.2023-11.2023
              <br>
              <strong>Meta</strong> (Redmond, US), AI Research Scientist Intern, 06.2022-09.2022
              <br>
              <strong>Microsoft Mixed Reality & AI Zurich Lab</strong> (Zurich, Switzerland), Research Intern, 10.2020-12.2020
            </td>
          </tr>
          
        </tbody></table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Academic service</heading>
            </td>
          </tr>
        </tbody></table>
        <table width="100%" align="center" border="0" cellpadding="20"><tbody>
          
          <tr>
            <td width="75%" valign="center">
              <strong>Journal reviewer</strong>: T-PAMI, IJCV, TIP, TVCG, RA-L, TCSVT, CAG, Neural Computing
              <br>
              <strong>Conference reviewer</strong>: CVPR, NeurIPS, ICCV, ECCV, AAAI, ICLR, ICML, ICRA, ACMMM, ACCV
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                Thanks for the <a href="https://github.com/jonbarron/jonbarron_website">awesome template</a>!
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
