[{"authors":null,"categories":null,"content":"I am a Ph.D. student in Computer Science at ETH Zurich, supervised by Prof. Marc Pollefeys. My research interests mainly lie in the field of 3D vision, reconstruction and deep learning. Previously, I obtained a Master\u0026rsquo;s degree in Robotics at ETH zurich with distinction. I also spent some time working on multi-view stereo at Microsoft Mixed Reality \u0026amp; AI Lab Zurich.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"I am a Ph.D. student in Computer Science at ETH Zurich, supervised by Prof. Marc Pollefeys. My research interests mainly lie in the field of 3D vision, reconstruction and deep learning.","tags":null,"title":"Fangjinhua Wang","type":"authors"},{"authors":["Fangjinhua Wang","Silvano Galliani","Christoph Vogel","Marc Pollefeys"],"categories":null,"content":"","date":1639008000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1639008000,"objectID":"7a4a6e96f8d156b91ab6c75bd4f8c25c","permalink":"https://fangjinhuawang.github.io/publication/itermvs/","publishdate":"2021-12-03T00:00:00Z","relpermalink":"/publication/itermvs/","section":"publication","summary":"We present IterMVS, a new data-driven method for high-resolution multi-view stereo. We propose a novel GRU-based estimator that encodes pixel-wise probability distributions of depth in its hidden state. Ingesting multi-scale matching information, our model refines these distributions over multiple iterations and infers depth and confidence. To extract the depth maps, we combine traditional classification and regression in a novel manner. We verify the efficiency and effectiveness of our method on DTU, Tanks\u0026Temples and ETH3D. While being the most efficient method in both memory and run-time, our model achieves competitive performance on DTU and better generalization ability on Tanks\u0026Temples as well as ETH3D than most state-of-the-art methods.","tags":null,"title":"IterMVS: Iterative Probability Estimation for Efficient Multi-View Stereo","type":"publication"},{"authors":["Nikhil Bharadwaj Gosala*","Fangjinhua Wang**","Zhaopeng Cui","Hanxue Liang","Oliver Glauser","Shihao Wu","Olga Sorkine-Hornung"],"categories":null,"content":"","date":1638230400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1638230400,"objectID":"9eacec2cf82877ca3c148b717e1211d3","permalink":"https://fangjinhuawang.github.io/publication/tvcg2021_self_calibrated_wearable/","publishdate":"2021-11-30T00:00:00Z","relpermalink":"/publication/tvcg2021_self_calibrated_wearable/","section":"publication","summary":"We present a multi-sensor system for consistent 3D hand pose tracking and modeling, which leverages the advantages of both wearable and optical sensors. Specifically, we employ a stretch-sensing soft glove and three IMUs in combination with an RGB-D camera. Different sensor modalities are fused based on the availability and confidence estimation, enabling seamless hand tracking in challenging environments with partial or even complete occlusion. To maximize the accuracy while maintaining a high ease-of-use, we propose an automated user calibration that uses the RGB-D camera data to refine both the glove mapping model and the multi-IMU system parameters. Extensive experiments show that our setup outperforms the wearable-only approaches when the hand is in the field-of-view and outplays the camera-only methods when the hand is occluded.","tags":null,"title":"Self-Calibrated Multi-Sensor Wearable for Hand Tracking and Modeling","type":"publication"},{"authors":["Fangjinhua Wang","Silvano Galliani","Christoph Vogel","Pablo Speciale","Marc Pollefeys"],"categories":null,"content":"","date":1606953600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1606953600,"objectID":"f0b126c90ac5776c2996624224515a69","permalink":"https://fangjinhuawang.github.io/publication/patchmatchnet/","publishdate":"2020-12-03T00:00:00Z","relpermalink":"/publication/patchmatchnet/","section":"publication","summary":"We present PatchmatchNet, a novel and learnable cascade formulation of Patchmatch for high-resolution multi-view stereo. With high computation speed and low memory requirement, PatchmatchNet can process higher resolution imagery and is more suited to run on resource limited devices than competitors that employ 3D cost volume regularization. For the first time we introduce an iterative multi-scale Patchmatch in an end-to-end trainable architecture and improve the Patchmatch core algorithm with a novel and learned adaptive propagation and evaluation scheme for each iteration. Extensive experiments show a very competitive performance and generalization for our method on DTU, Tanks \u0026 Temples and ETH3D, but at a significantly higher efficiency than all existing top-performing models, at least two and a half times faster than state-of-the-art methods with twice less memory usage.","tags":null,"title":"PatchmatchNet: Learned Multi-View Patchmatch Stereo","type":"publication"},{"authors":[],"categories":[],"content":"Create slides in Markdown with Wowchemy Wowchemy | Documentation\n Features  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides   Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export: E   Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026quot;blueberry\u0026quot; if porridge == \u0026quot;blueberry\u0026quot;: print(\u0026quot;Eating...\u0026quot;)   Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = ;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\n Fragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}}  Press Space to play!\nOne  Two  Three \n A fragment can accept two optional parameters:\n class: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears   Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}}  Press the S key to view the speaker notes!\n Only the speaker can read these notes Press S key to view    Themes  black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links    night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links   Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026quot;/media/boards.jpg\u0026quot; \u0026gt;}} {{\u0026lt; slide background-color=\u0026quot;#0000FF\u0026quot; \u0026gt;}} {{\u0026lt; slide class=\u0026quot;my-style\u0026quot; \u0026gt;}}   Custom CSS Example Let\u0026rsquo;s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; }   Questions? Ask\nDocumentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"https://fangjinhuawang.github.io/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"An introduction to using Wowchemy's Slides feature.","tags":[],"title":"Slides","type":"slides"},{"authors":["Gaoyang Pang","Jia Deng","Fangjinhua Wang","Junhui Zhang","Zhibo Pang","Geng Yang"],"categories":null,"content":"","date":1541030400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1541030400,"objectID":"fac40fa3e4089b5e612f0779c5166412","permalink":"https://fangjinhuawang.github.io/publication/micromachine2018/","publishdate":"2018-01-01T00:00:00Z","relpermalink":"/publication/micromachine2018/","section":"publication","summary":"","tags":["Source Themes"],"title":"Development of Flexible Robot Skin for Safe and Natural Humanâ€“Robot Collaboration","type":"publication"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"f26b5133c34eec1aa0a09390a36c2ade","permalink":"https://fangjinhuawang.github.io/admin/config.yml","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/admin/config.yml","section":"","summary":"","tags":null,"title":"","type":"wowchemycms"}]